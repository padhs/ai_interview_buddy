package handlers

import (
	"bytes"
	"context"
	"encoding/base64"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"mime"
	"mime/multipart"
	"net/http"
	"os"
	"path/filepath"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/google/generative-ai-go/genai"
	"github.com/padhs/ai_interview_buddy/backend/internal/types"
	"google.golang.org/api/option"

	speech "cloud.google.com/go/speech/apiv1"
	speechpb "google.golang.org/genproto/googleapis/cloud/speech/v1"
)

var (
	promptTemplateOnce sync.Once
	promptTemplate     string
)

func getPromptTemplate() string {
	promptTemplateOnce.Do(func() {
		// Read prompt.text from vision package (relative to source location)
		// Try multiple paths for flexibility
		paths := []string{
			filepath.Join("..", "vision", "prompt.text"),                   // relative from handlers/
			filepath.Join("internal", "services", "vision", "prompt.text"), // from module root
		}
		for _, p := range paths {
			if data, err := os.ReadFile(p); err == nil {
				promptTemplate = string(data)
				return
			}
		}
		// Fallback prompt if file not found
		promptTemplate = `You are an AI Interviewer Agent for a technical screening platform. 
Your persona is that of 'Alex', a friendly, patient, and encouraging Senior Software Engineer. 
Be concise, use the Socratic method, and maintain a positive tone.`
	})
	return promptTemplate
}

// POST /api/v1/voice/ingest
// Accepts audio via multipart/form-data (field name: "audio") or raw body with Content-Type: audio/*
// Query params (optional): encoding=WEBM_OPUS|LINEAR16, sampleRate=48000, language=en-US
func VoiceIngest(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), 90*time.Second)
	defer cancel()

	var audioBytes []byte
	ctype := r.Header.Get("Content-Type")
	mt, _, _ := mime.ParseMediaType(ctype)
	if strings.HasPrefix(mt, "multipart/") {
		// read multipart file part 'audio'
		if err := r.ParseMultipartForm(32 << 20); err != nil {
			http.Error(w, "invalid multipart form", http.StatusBadRequest)
			return
		}
		var file multipart.File
		var err error
		file, _, err = r.FormFile("audio")
		if err != nil {
			http.Error(w, "missing audio file part", http.StatusBadRequest)
			return
		}
		defer file.Close()
		audioBytes, err = io.ReadAll(file)
		if err != nil {
			http.Error(w, "failed to read audio", http.StatusBadRequest)
			return
		}
	} else if strings.HasPrefix(mt, "audio/") || mt == "application/octet-stream" || mt == "" {
		// raw body
		data, err := io.ReadAll(http.MaxBytesReader(w, r.Body, 25<<20))
		if err != nil {
			http.Error(w, "failed to read audio", http.StatusBadRequest)
			return
		}
		audioBytes = data
	} else {
		http.Error(w, "unsupported content-type", http.StatusUnsupportedMediaType)
		return
	}

	// Build STT request
	encStr := r.URL.Query().Get("encoding")
	if encStr == "" {
		encStr = "WEBM_OPUS"
	}
	var enc speechpb.RecognitionConfig_AudioEncoding
	switch strings.ToUpper(encStr) {
	case "LINEAR16":
		enc = speechpb.RecognitionConfig_LINEAR16
	case "OGG_OPUS":
		enc = speechpb.RecognitionConfig_OGG_OPUS
	case "WEBM_OPUS":
		enc = speechpb.RecognitionConfig_WEBM_OPUS
	default:
		enc = speechpb.RecognitionConfig_WEBM_OPUS
	}
	sr := int32(48000)
	if s := r.URL.Query().Get("sampleRate"); s != "" {
		if v, err := strconv.Atoi(s); err == nil {
			sr = int32(v)
		}
	}
	lang := r.URL.Query().Get("language")
	if lang == "" {
		lang = "en-US"
	}

	client, err := speech.NewClient(ctx)
	if err != nil {
		http.Error(w, "speech client init failed", http.StatusInternalServerError)
		return
	}
	defer client.Close()

	req := &speechpb.RecognizeRequest{
		Config: &speechpb.RecognitionConfig{
			Encoding:                   enc,
			SampleRateHertz:            sr,
			LanguageCode:               lang,
			EnableAutomaticPunctuation: true,
		},
		Audio: &speechpb.RecognitionAudio{AudioSource: &speechpb.RecognitionAudio_Content{Content: audioBytes}},
	}
	resp, err := client.Recognize(ctx, req)
	if err != nil {
		http.Error(w, fmt.Sprintf("recognize failed: %v", err), http.StatusBadGateway)
		return
	}
	var transcript strings.Builder
	for _, result := range resp.GetResults() {
		if len(result.GetAlternatives()) > 0 {
			if transcript.Len() > 0 {
				transcript.WriteString(" ")
			}
			transcript.WriteString(result.GetAlternatives()[0].GetTranscript())
		}
	}
	w.Header().Set("Content-Type", "application/json")
	_ = json.NewEncoder(w).Encode(types.VoiceIngestResponse{Transcript: transcript.String()})
}

// POST /api/v1/voice/tts { text: string, voiceId: string }
// Streams audio/mpeg generated by ElevenLabs
func VoiceTTS(w http.ResponseWriter, r *http.Request) {
	var body types.VoiceTTSRequest
	if err := json.NewDecoder(r.Body).Decode(&body); err != nil || strings.TrimSpace(body.Text) == "" {
		http.Error(w, "invalid body", http.StatusBadRequest)
		return
	}
	// Use environment variable as fallback if voice ID not provided in request
	if body.VoiceID == "" {
		body.VoiceID = strings.TrimSpace(os.Getenv("ELEVENLABS_VOICE_ID"))
		if body.VoiceID == "" {
			body.VoiceID = "21m00Tcm4TlvDq8ikWAM" // final fallback Rachel voice
		}
	}
	if body.ModelID == "" {
		body.ModelID = "eleven_multilingual_v2"
	}

	apiKey := os.Getenv("ELEVENLABS_API_KEY")
	if apiKey == "" {
		http.Error(w, "missing ELEVENLABS_API_KEY", http.StatusInternalServerError)
		return
	}
	url := "https://api.elevenlabs.io/v1/text-to-speech/" + body.VoiceID + "/stream"
	payload, _ := json.Marshal(map[string]any{
		"text":     body.Text,
		"model_id": body.ModelID,
		"voice_settings": map[string]interface{}{
			"stability":         0.4,
			"similarity_boost":  0.8,
			"style":             0.5,
			"use_speaker_boost": true,
		},
	})
	req, _ := http.NewRequest("POST", url, bytes.NewReader(payload))
	req.Header.Set("xi-api-key", apiKey)
	req.Header.Set("accept", "audio/mpeg")
	req.Header.Set("content-type", "application/json")
	resp, err := (&http.Client{Timeout: 60 * time.Second}).Do(req)
	if err != nil {
		http.Error(w, "tts request failed", http.StatusBadGateway)
		return
	}
	defer resp.Body.Close()
	if resp.StatusCode < 200 || resp.StatusCode >= 300 {
		b, _ := io.ReadAll(resp.Body)
		http.Error(w, fmt.Sprintf("tts error: %d %s", resp.StatusCode, string(b)), http.StatusBadGateway)
		return
	}
	w.Header().Set("Content-Type", "audio/mpeg")
	w.WriteHeader(http.StatusOK)
	io.Copy(w, resp.Body)
}

// POST /api/v1/voice/chat
// Accepts audio (webm/opus or wav) + optional UI context JSON
// Sends audio to Gemini for STT + processing, then TTS via ElevenLabs, returns audio/mpeg
func VoiceChat(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), 60*time.Second)
	defer cancel()

	// Parse multipart form: audio file + optional context JSON
	if err := r.ParseMultipartForm(25 << 20); err != nil {
		http.Error(w, "invalid multipart form", http.StatusBadRequest)
		return
	}

	// Get audio file
	file, _, err := r.FormFile("audio")
	if err != nil {
		http.Error(w, "missing audio file", http.StatusBadRequest)
		return
	}
	defer file.Close()

	audioBytes, err := io.ReadAll(file)
	if err != nil || len(audioBytes) == 0 {
		http.Error(w, "empty audio", http.StatusBadRequest)
		return
	}

	// Get optional UI context
	var uiContext *types.ObserveUIState
	if contextJSON := r.FormValue("context"); contextJSON != "" {
		var ui types.ObserveUIState
		if err := json.Unmarshal([]byte(contextJSON), &ui); err == nil {
			uiContext = &ui
		}
	}

	// Get optional screenshot
	var screenshotBytes []byte
	if screenshotBase64 := r.FormValue("screenshot"); screenshotBase64 != "" {
		if decoded, err := base64.StdEncoding.DecodeString(screenshotBase64); err == nil {
			screenshotBytes = decoded
		}
	}

	// Get mime type from form file or default to webm
	mimeType := "audio/webm" // MediaRecorder typically produces webm/opus

	// Call Gemini with audio + screenshot + context -> get text response
	geminiText, err := callGeminiWithAudio(ctx, audioBytes, mimeType, screenshotBytes, uiContext)
	if err != nil {
		http.Error(w, "gemini error: "+err.Error(), http.StatusBadGateway)
		return
	}

	// Convert text to speech via ElevenLabs
	audioMP3, err := synthesizeSpeechToBytes(ctx, geminiText)
	if err != nil {
		http.Error(w, "tts error: "+err.Error(), http.StatusBadGateway)
		return
	}

	// Return audio/mpeg
	w.Header().Set("Content-Type", "audio/mpeg")
	w.WriteHeader(http.StatusOK)
	w.Write(audioMP3)
}

// callGeminiWithAudio sends audio to Gemini for transcription + processing
func callGeminiWithAudio(ctx context.Context, audioBytes []byte, mimeType string, screenshotBytes []byte, uiContext *types.ObserveUIState) (string, error) {
	apiKey := os.Getenv("GEMINI_API_KEY")
	if apiKey == "" {
		return "", errors.New("missing GEMINI_API_KEY")
	}

	client, err := genai.NewClient(ctx, option.WithAPIKey(apiKey))
	if err != nil {
		return "", err
	}
	defer client.Close()

	model := client.GenerativeModel("gemini-2.5-flash")

	// Use the same prompt template as vision
	sys := getPromptTemplate()
	model.SystemInstruction = &genai.Content{Parts: []genai.Part{genai.Text(sys)}}

	// Build user content: audio + optional screenshot + optional UI context
	parts := []genai.Part{
		&genai.Blob{MIMEType: mimeType, Data: audioBytes},
	}

	if len(screenshotBytes) > 0 {
		parts = append(parts, &genai.Blob{MIMEType: "image/webp", Data: screenshotBytes})
	}

	if uiContext != nil {
		uiJSON, _ := json.Marshal(uiContext)
		parts = append(parts, genai.Text("UI_STATE_JSON:\n"+string(uiJSON)))
	}

	resp, err := model.GenerateContent(ctx, parts...)
	if err != nil || len(resp.Candidates) == 0 {
		return "", err
	}

	// Extract text response
	var txt string
	for _, part := range resp.Candidates[0].Content.Parts {
		if t, ok := part.(genai.Text); ok {
			txt += string(t)
		}
	}

	if strings.TrimSpace(txt) == "" {
		txt = "Could you walk me through your current approach?"
	}

	return txt, nil
}

// synthesizeSpeechToBytes calls ElevenLabs TTS and returns MP3 bytes
func synthesizeSpeechToBytes(ctx context.Context, text string) ([]byte, error) {
	apiKey := os.Getenv("ELEVENLABS_API_KEY")
	voiceID := strings.TrimSpace(os.Getenv("ELEVENLABS_VOICE_ID"))
	if voiceID == "" {
		voiceID = "Eric" // default
	}

	if apiKey == "" {
		return nil, errors.New("missing ELEVENLABS_API_KEY")
	}

	// Log voice ID being used for debugging (remove in production if needed)
	// fmt.Printf("[DEBUG] Using ElevenLabs voice ID: %s\n", voiceID)

	payload := map[string]any{
		"text":     text,
		"model_id": "eleven_multilingual_v2",
		"voice_settings": map[string]interface{}{
			"stability":         0.4,
			"similarity_boost":  0.8,
			"style":             0.5,
			"use_speaker_boost": true,
		},
	}
	jsonBody, _ := json.Marshal(payload)

	req, _ := http.NewRequestWithContext(ctx, "POST",
		"https://api.elevenlabs.io/v1/text-to-speech/"+voiceID+"/stream",
		bytes.NewReader(jsonBody))
	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("xi-api-key", apiKey)
	req.Header.Set("accept", "audio/mpeg")

	resp, err := (&http.Client{Timeout: 30 * time.Second}).Do(req)
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		b, _ := io.ReadAll(resp.Body)
		return nil, fmt.Errorf("tts_failed: %d - voice_id used: '%s' - %s", resp.StatusCode, voiceID, string(b))
	}

	return io.ReadAll(resp.Body)
}
